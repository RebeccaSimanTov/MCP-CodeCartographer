"""
LLM Factory - Create optimized LangChain LLM instances
××™××•×© ××œ× ×©×œ ×™×¦×™×¨×ª LLM ×¢× ×›×œ ×”××•×¤×˜×™××™×–×¦×™×•×ª
"""
from typing import Optional, Dict, Any
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.language_models import BaseChatModel
from langchain_core.callbacks import CallbackManager, StdOutCallbackHandler

from src.llm.llm_config import LLMConfig, LLMProvider


class LLMFactory:
    """
    Factory ××ª×§×“× ×œ×™×¦×™×¨×ª LLM instances
    ×ª×•××š ×‘×›×œ ×”×ª×›×•× ×•×ª ×”××ª×§×“××•×ª ×©×œ LangChain
    """
    
    @staticmethod
    def create_llm(
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        streaming: bool = False,
        callbacks: Optional[list] = None,
        **kwargs
    ) -> BaseChatModel:
        """
        ×™×¦×™×¨×ª LLM instance ×××•×¤×˜××–
        
        Args:
            provider: ×¡×¤×§ LLM ("gemini", "openai", "anthropic")
            model: ×©× ××•×“×œ (None = default)
            temperature: temperature (None = default)
            streaming: ×”×× ×œ××¤×©×¨ streaming
            callbacks: callbacks ×œ-monitoring
            **kwargs: ×¤×¨××˜×¨×™× × ×•×¡×¤×™×
            
        Returns:
            BaseChatModel: instance ××•×›×Ÿ ×œ×©×™××•×©
        """
        # Default provider
        if provider is None:
            provider = LLMConfig.DEFAULT_PROVIDER
        
        # Validate provider
        if not LLMConfig.validate_provider(provider):
            available = LLMConfig.list_available_providers()
            raise ValueError(
                f"âŒ Provider '{provider}' is not configured!\n"
                f"Available providers: {available}\n"
                f"Please add the required API key to .env"
            )
        
        # Get configuration
        config = LLMConfig.get_model_config(provider)
        api_key = LLMConfig.get_api_key(provider)
        model_name = model or config["model"]
        temp = temperature if temperature is not None else config["temperature"]
        
        # Setup callbacks
        if callbacks is None:
            callbacks = []
        
        callback_manager = CallbackManager(callbacks) if callbacks else None
        
        print(f"ğŸ¤– Creating {provider.upper()} LLM:")
        print(f"   ğŸ“¦ Model: {model_name}")
        print(f"   ğŸŒ¡ï¸  Temperature: {temp}")
        print(f"   ğŸ“¡ Streaming: {streaming}")
        
        # Create LLM based on provider
        if provider == "gemini":
            return ChatGoogleGenerativeAI(
                model=model_name,
                temperature=temp,
                google_api_key=api_key,
                streaming=streaming,
                callbacks=callback_manager,
                convert_system_message_to_human=True,
                max_output_tokens=config.get("max_tokens", 8192),
                timeout=config.get("timeout", 60),
                **kwargs
            )
        
        elif provider == "openai":
            return ChatOpenAI(
                model=model_name,
                temperature=temp,
                api_key=api_key,
                streaming=streaming,
                callbacks=callback_manager,
                max_tokens=config.get("max_tokens", 4096),
                timeout=config.get("timeout", 60),
                **kwargs
            )
        
        elif provider == "anthropic":
            return ChatAnthropic(
                model=model_name,
                temperature=temp,
                api_key=api_key,
                streaming=streaming,
                callbacks=callback_manager,
                max_tokens=config.get("max_tokens", 4096),
                timeout=config.get("timeout", 60),
                **kwargs
            )
        
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    @staticmethod
    def create_json_llm(
        provider: Optional[LLMProvider] = None,
        **kwargs
    ) -> BaseChatModel:
        """
        ×™×¦×™×¨×ª LLM ×¢× JSON mode (structured output)
        
        Args:
            provider: ×¡×¤×§ LLM
            **kwargs: ×¤×¨××˜×¨×™× × ×•×¡×¤×™×
            
        Returns:
            BaseChatModel: LLM ××•×’×“×¨ ×œ-JSON output
        """
        if provider is None:
            provider = LLMConfig.DEFAULT_PROVIDER
        
        print(f"ğŸ“‹ Creating JSON-mode LLM for {provider.upper()}")
        
        if provider == "gemini":
            # Gemini uses generation_config
            return LLMFactory.create_llm(
                provider=provider,
                **kwargs
            )
        
        elif provider == "openai":
            # OpenAI uses response_format
            return LLMFactory.create_llm(
                provider=provider,
                model_kwargs={"response_format": {"type": "json_object"}},
                **kwargs
            )
        
        elif provider == "anthropic":
            # Claude can use tool calling for structured output
            return LLMFactory.create_llm(
                provider=provider,
                **kwargs
            )
        
        return LLMFactory.create_llm(provider=provider, **kwargs)
    
    @staticmethod
    def create_streaming_llm(
        provider: Optional[LLMProvider] = None,
        on_token: Optional[callable] = None,
        **kwargs
    ) -> BaseChatModel:
        """
        ×™×¦×™×¨×ª LLM ×¢× streaming support
        
        Args:
            provider: ×¡×¤×§ LLM
            on_token: callback ×œ×›×œ token
            **kwargs: ×¤×¨××˜×¨×™× × ×•×¡×¤×™×
            
        Returns:
            BaseChatModel: LLM ×¢× streaming
        """
        from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
        
        callbacks = []
        if on_token:
            # Custom callback
            class TokenCallback(StreamingStdOutCallbackHandler):
                def on_llm_new_token(self, token: str, **kwargs):
                    on_token(token)
            callbacks.append(TokenCallback())
        else:
            callbacks.append(StreamingStdOutCallbackHandler())
        
        return LLMFactory.create_llm(
            provider=provider,
            streaming=True,
            callbacks=callbacks,
            **kwargs
        )
    
    @staticmethod
    def get_provider_info(provider: Optional[LLMProvider] = None) -> Dict[str, Any]:
        """
        ×§×‘×œ ××™×“×¢ ×¢×œ ×¡×¤×§
        
        Args:
            provider: ×¡×¤×§ LLM
            
        Returns:
            Dict ×¢× ××™×“×¢
        """
        if provider is None:
            provider = LLMConfig.DEFAULT_PROVIDER
        
        config = LLMConfig.get_model_config(provider)
        
        return {
            "provider": provider,
            "model": config["model"],
            "temperature": config["temperature"],
            "max_tokens": config.get("max_tokens", "unknown"),
            "structured_output": config.get("supports_structured_output", False),
            "api_key_configured": LLMConfig.validate_provider(provider)
        }